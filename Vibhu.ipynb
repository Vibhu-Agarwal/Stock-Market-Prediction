{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:29:02.583832Z",
     "start_time": "2019-02-01T13:29:01.916611Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:29:03.183842Z",
     "start_time": "2019-02-01T13:29:03.169351Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:29:04.070843Z",
     "start_time": "2019-02-01T13:29:04.043169Z"
    }
   },
   "outputs": [],
   "source": [
    "class StockDataSet(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 stock_sym=\"GSPC.csv\",\n",
    "                 input_size=1,\n",
    "                 num_steps=30,\n",
    "                 test_ratio=0.1,\n",
    "                 normalized=True,\n",
    "                 close_price_only=True):\n",
    "        self.stock_sym = stock_sym\n",
    "        self.input_size = input_size\n",
    "        self.num_steps = num_steps\n",
    "        self.test_ratio = test_ratio\n",
    "        self.close_price_only = close_price_only\n",
    "        self.normalized = normalized\n",
    "\n",
    "        self.raw_df = self.read_dataset()\n",
    "\n",
    "        # Merge into one sequence\n",
    "        if close_price_only:\n",
    "            self.raw_seq = self.raw_df['Close']\n",
    "        else:\n",
    "            self.raw_seq = [price for tup in self.raw_df[['Open', 'Close']].values for price in tup]\n",
    "\n",
    "        self.raw_seq = np.array(self.raw_seq)\n",
    "        self.train_X, self.train_y, self.test_X, self.test_y = self._prepare_data(self.raw_seq)\n",
    "\n",
    "        \n",
    "    def read_dataset(self):\n",
    "        cwd = os.getcwd()\n",
    "        dataset_path = os.path.join(cwd, self.stock_sym)\n",
    "        data = pd.read_csv(dataset_path, parse_dates=['Date'], infer_datetime_format=True)\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def _prepare_data(self, seq):\n",
    "        # split into items of input_size\n",
    "        seq = [np.array(seq[i * self.input_size: (i + 1) * self.input_size])\n",
    "               for i in range(len(seq) // self.input_size)]\n",
    "\n",
    "        if self.normalized:\n",
    "            seq = [seq[0] / seq[0][0] - 1.0] + [\n",
    "                curr / seq[i][-1] - 1.0 for i, curr in enumerate(seq[1:])]\n",
    "\n",
    "        # split into groups of num_steps\n",
    "        X = np.array([seq[i: i + self.num_steps] for i in range(len(seq) - self.num_steps)])\n",
    "        y = np.array([seq[i + self.num_steps] for i in range(len(seq) - self.num_steps)])\n",
    "\n",
    "        train_size = int(len(X) * (1.0 - self.test_ratio))\n",
    "        train_X, test_X = X[:train_size], X[train_size:]\n",
    "        train_y, test_y = y[:train_size], y[train_size:]\n",
    "        return train_X, train_y, test_X, test_y\n",
    "    \n",
    "\n",
    "    def generate_one_epoch(self, batch_size):\n",
    "        num_batches = int(len(self.train_X)) // batch_size\n",
    "        if batch_size * num_batches < len(self.train_X):\n",
    "            num_batches += 1\n",
    "\n",
    "        batch_indices = list(range(num_batches))\n",
    "        random.shuffle(batch_indices)\n",
    "        for j in batch_indices:\n",
    "            batch_X = self.train_X[j * batch_size : (j + 1) * batch_size]\n",
    "            batch_y = self.train_y[j * batch_size : (j + 1) * batch_size]\n",
    "            assert set(map(len, batch_X)) == {self.num_steps}\n",
    "            yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:29:04.805270Z",
     "start_time": "2019-02-01T13:29:04.700693Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = StockDataSet().raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:31.628420Z",
     "start_time": "2019-02-01T13:25:31.593217Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:32.281630Z",
     "start_time": "2019-02-01T13:25:32.193249Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:32.671548Z",
     "start_time": "2019-02-01T13:25:32.651327Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:33.593578Z",
     "start_time": "2019-02-01T13:25:33.519230Z"
    }
   },
   "outputs": [],
   "source": [
    "time_n_closing_value = dataset.iloc[:,[0,4]]\n",
    "time_n_closing_value['Date']=pd.to_datetime(time_n_closing_value['Date'], format='%d/%m/%Y')\n",
    "time_n_closing_value.set_index(['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:34.960741Z",
     "start_time": "2019-02-01T13:25:34.345683Z"
    }
   },
   "outputs": [],
   "source": [
    "time_n_closing_value.plot()\n",
    "plt.title('Date Vs Close Price Graph')\n",
    "plt.ylabel('Close Price')\n",
    "plt.xlabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:36.813519Z",
     "start_time": "2019-02-01T13:25:36.808094Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNConfig():\n",
    "    input_size=3\n",
    "    num_steps=30\n",
    "    lstm_size=128\n",
    "    num_layers=1\n",
    "    keep_prob=0.8\n",
    "    batch_size = 64\n",
    "    init_learning_rate = 0.005\n",
    "    learning_rate_decay = 0.99\n",
    "    init_epoch = 5\n",
    "    max_epoch = 100\n",
    "\n",
    "config = RNNConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:39.463352Z",
     "start_time": "2019-02-01T13:25:38.433559Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:40.260595Z",
     "start_time": "2019-02-01T13:25:40.208817Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_lstm_graph():\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    lstm_graph = tf.Graph()\n",
    "\n",
    "    with lstm_graph.as_default():\n",
    "\n",
    "        inputs = tf.placeholder(tf.float32, [None, config.num_steps, config.input_size], name=\"Inputs\")\n",
    "        targets = tf.placeholder(tf.float32, [None, config.input_size], name=\"Targets\")\n",
    "        learning_rate = tf.placeholder(tf.float32, None, name=\"learn_rate\")\n",
    "\n",
    "        def _create_one_cell():\n",
    "            lstm_cell = tf.contrib.rnn.LSTMCell(config.lstm_size, state_is_tuple=True)\n",
    "            if config.keep_prob < 1.0:\n",
    "                lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n",
    "            return lstm_cell\n",
    "\n",
    "        if config.num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                [_create_one_cell() for _ in range(config.num_layers)],\n",
    "                state_is_tuple=True)\n",
    "        else:\n",
    "            cell = _create_one_cell()\n",
    "\n",
    "        val, _ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, scope=\"RNN\")\n",
    "        val = tf.transpose(val, [1,0,2])\n",
    "\n",
    "        with tf.name_scope(\"Output_Layer\"):\n",
    "            last = tf.gather(val, int(val.get_shape()[0]) - 1, name=\"Last_LSTM_Outputs\")\n",
    "\n",
    "            weight = tf.Variable(tf.truncated_normal([config.lstm_size, config.input_size]), name=\"weights\")\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[config.input_size]), name=\"biases\")\n",
    "            prediction = tf.matmul(last, weight) + bias\n",
    "\n",
    "            tf.summary.histogram(\"Last_LSTM_Outputs\", last)\n",
    "            tf.summary.histogram(\"weights\", weight)\n",
    "            tf.summary.histogram(\"biases\", bias)\n",
    "\n",
    "        with tf.name_scope(\"Train_Attrs\"):\n",
    "            loss = tf.reduce_mean(tf.square(prediction-targets), name=\"MSE_Loss\")\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            minimize = optimizer.minimize(loss, name=\"loss_adam_minimize\")\n",
    "            tf.summary.scalar(\"MSE_Loss\", loss)\n",
    "\n",
    "        for op in [prediction, loss]:\n",
    "            tf.add_to_collection(\"Ops_to_Restore\", op)\n",
    "    \n",
    "    return lstm_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:41.332369Z",
     "start_time": "2019-02-01T13:25:41.315207Z"
    }
   },
   "outputs": [],
   "source": [
    "def _compute_learning_rates():\n",
    "    learning_rates_to_use = [config.init_learning_rate * (config.learning_rate_decay ** max(float(i + 1 - config.init_epoch), 0.0)) for i in range(config.max_epoch)]\n",
    "    print(\"Middle learning rate:\", learning_rates_to_use[len(learning_rates_to_use) // 2])\n",
    "    return learning_rates_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:42.250259Z",
     "start_time": "2019-02-01T13:25:42.215085Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_lstm_graph(lstm_graph, stock_name=\"GSPC.csv\", norm=True):\n",
    "    \n",
    "    stock_dataset = StockDataSet(stock_name, config.input_size, num_steps=config.num_steps, normalized=norm)\n",
    "    \n",
    "    final_prediction = []\n",
    "    final_loss = None\n",
    "    \n",
    "    graph_name = \"{sn}_lr={lr}_lr-decay={lrd}_lstm-size={size}_num-steps={steps}_input-size={inp}_batch-size={bs}_epochs={ep}\"\n",
    "    graph_name = graph_name.format(sn=stock_name[:stock_name.find('.')],\n",
    "                     lr=config.init_learning_rate,\n",
    "                     lrd=config.learning_rate_decay,\n",
    "                     size=config.lstm_size,\n",
    "                     steps=config.num_steps,\n",
    "                     inp=config.input_size,\n",
    "                     bs=config.batch_size,\n",
    "                     ep=config.max_epoch)\n",
    "    \n",
    "    print(graph_name)\n",
    "    \n",
    "    learning_rates_to_use = _compute_learning_rates()\n",
    "    #print('learning rates:\\n', learning_rates_to_use)\n",
    "    with tf.Session(graph=lstm_graph) as sess:\n",
    "        \n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        print('merged_summary:',type(merged_summary))\n",
    "        writer = tf.summary.FileWriter('logs/' + graph_name)\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        inputs = graph.get_tensor_by_name('Inputs:0')\n",
    "        targets = graph.get_tensor_by_name('Targets:0')\n",
    "        learning_rate = graph.get_tensor_by_name('learn_rate:0')\n",
    "\n",
    "        test_data_feed = {\n",
    "            inputs: stock_dataset.test_X,\n",
    "            targets: stock_dataset.test_y,\n",
    "            learning_rate: 0.0\n",
    "        }\n",
    "\n",
    "        loss = graph.get_tensor_by_name('Train_Attrs/MSE_Loss:0')\n",
    "        minimize = graph.get_operation_by_name('Train_Attrs/loss_adam_minimize')\n",
    "        prediction = graph.get_tensor_by_name('Output_Layer/add:0')\n",
    "\n",
    "        for epoch_step in range(config.max_epoch):\n",
    "            current_lr = learning_rates_to_use[epoch_step]\n",
    "\n",
    "            for batch_X, batch_y in stock_dataset.generate_one_epoch(config.batch_size):\n",
    "                train_data_feed = {\n",
    "                    inputs: batch_X,\n",
    "                    targets: batch_y,\n",
    "                    learning_rate: current_lr\n",
    "                }\n",
    "                train_loss, _ = sess.run([loss, minimize], train_data_feed)\n",
    "\n",
    "            if epoch_step % 10 == 0:\n",
    "                test_loss, _pred, _summary = sess.run([loss, prediction, merged_summary], test_data_feed)\n",
    "                assert len(_pred) == len(stock_dataset.test_y)\n",
    "                print_out = \"Epoch {epoch_step} lr={lr} test_loss={test_loss}\".format(epoch_step=epoch_step,\n",
    "                                                                                     lr=current_lr,\n",
    "                                                                                     test_loss=test_loss)\n",
    "                print(print_out)\n",
    "                if epoch_step % 50 == 0:\n",
    "                    print(\"Predictions:\", [(\n",
    "                        map(lambda x: round(x, 4), _pred[-j]),\n",
    "                        map(lambda x: round(x, 4), stock_dataset.test_y[-j])\n",
    "                    ) for j in range(5)])\n",
    "\n",
    "            writer.add_summary(_summary, global_step=epoch_step)\n",
    "\n",
    "        print(\"Final Results:\")\n",
    "        final_prediction, final_loss = sess.run([prediction, loss], test_data_feed)\n",
    "        print(final_prediction, final_loss)\n",
    "\n",
    "        graph_saver_dir = os.path.join('models', graph_name)\n",
    "        if not os.path.exists(graph_saver_dir):\n",
    "            os.mkdir(graph_saver_dir)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, os.path.join(graph_saver_dir, \"stock_rnn_model_{gn}.ckpt\".format(gn=graph_name)),\n",
    "                                      global_step=epoch_step)\n",
    "\n",
    "    with open(\"final_predictions.{}.json\".format(graph_name), 'w') as fout:\n",
    "        fout.write(json.dumps(final_prediction.tolist()))\n",
    "    \n",
    "    return graph_name, stock_dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:25:49.464388Z",
     "start_time": "2019-02-01T13:25:43.496934Z"
    }
   },
   "outputs": [],
   "source": [
    "lstm_graph = create_lstm_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:26:02.737057Z",
     "start_time": "2019-02-01T13:25:50.780689Z"
    }
   },
   "outputs": [],
   "source": [
    "trained_graph, stock_dataset = train_lstm_graph(lstm_graph, norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restoring Model and Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:26:08.192410Z",
     "start_time": "2019-02-01T13:26:08.184448Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:26:09.833585Z",
     "start_time": "2019-02-01T13:26:09.819576Z"
    }
   },
   "outputs": [],
   "source": [
    "def prediction_of_model(graph_name, max_epoch, test_X, test_y):\n",
    "    test_prediction = None\n",
    "    test_loss = None\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        graph_path = os.path.join('models',graph_name,'stock_rnn_model_{graph}.ckpt-{epochs}.meta'.format(\n",
    "            graph=graph_name, epochs=max_epoch-1))\n",
    "        checkpoint_path = os.path.join('models', graph_name)\n",
    "        \n",
    "        importer = tf.train.import_meta_graph(graph_path)\n",
    "        importer.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "\n",
    "        inputs = graph.get_tensor_by_name('Inputs:0')\n",
    "        targets = graph.get_tensor_by_name('Targets:0')\n",
    "        learning_rate = graph.get_tensor_by_name('learn_rate:0')\n",
    "\n",
    "        test_data = {\n",
    "            inputs: test_X,\n",
    "            targets: test_y,\n",
    "            learning_rate: 0.0\n",
    "        }\n",
    "        \n",
    "        loss = graph.get_tensor_by_name('Train_Attrs/MSE_Loss:0')\n",
    "        prediction = graph.get_tensor_by_name('Output_Layer/add:0')\n",
    "        \n",
    "        test_prediction, test_loss = sess.run([prediction, loss], test_data)\n",
    "        \n",
    "    return test_prediction, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:26:10.487059Z",
     "start_time": "2019-02-01T13:26:10.470984Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((stock_dataset.train_X, stock_dataset.test_X), axis=0)\n",
    "Y = np.concatenate((stock_dataset.train_y, stock_dataset.test_y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:26:11.093780Z",
     "start_time": "2019-02-01T13:26:11.081545Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:26:11.927873Z",
     "start_time": "2019-02-01T13:26:11.620147Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res, loss = prediction_of_model(trained_graph, config.max_epoch, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:26:13.680311Z",
     "start_time": "2019-02-01T13:26:13.676123Z"
    }
   },
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:26:16.712915Z",
     "start_time": "2019-02-01T13:26:16.707479Z"
    }
   },
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:26:18.688215Z",
     "start_time": "2019-02-01T13:26:18.683213Z"
    }
   },
   "outputs": [],
   "source": [
    "result_size = res.shape[0]*res.shape[1]\n",
    "res_1d = np.ravel(res)[int(result_size/result_size+1):]\n",
    "Y_1d = np.ravel(Y)[int(result_size/result_size+1):]\n",
    "X_1d = np.array(list(range(Y_1d.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T13:26:21.611380Z",
     "start_time": "2019-02-01T13:26:20.879869Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_1d, Y_1d,c='r', label='Actual Data')\n",
    "plt.plot(X_1d, res_1d,c='b', label='Predictions')\n",
    "plt.ylim(-Y_1d.max()*0.8, Y_1d.max())\n",
    "plt.savefig(trained_graph+'.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "281px",
    "width": "240px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "363.85px",
    "left": "808px",
    "right": "105px",
    "top": "4px",
    "width": "453px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
